@article{Iskander2018,
   author = {Magued Iskander},
   doi = {10.1016/B978-0-12-803139-1.00003-5},
   isbn = {9780128031391},
   title = {Geotechnical Underground Sensing and Monitoring},
   url = {http://dx.doi.org/10.1016/B978-0-12-803139-1.00003-5},
   year = {2018}
}
@article{Xiao2018,
   author = {Ya-Xun Xiao and Xia-Ting Feng and Bing-Rui Chen and Guangliang Feng},
   doi = {10.1016/B978-0-12-805054-5.00009-3},
   isbn = {9780128050545},
   title = {Microseismic Monitoring Method of the Rockburst Evolution Process},
   url = {https://doi.org/10.1016/B978-0-12-805054-5.00009-3},
   year = {2018}
}
@article{,
   title = {Preston New Road-1z: LJ/06-09(z) HFP Report Preston New Road-1z HFP Report Proposed date of Review}
}
@article{Song2020,
   abstract = {Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by "early stopping" training a deep neu-ral network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a "maximal safe set," which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4-8.2 percent points under the existence of real-world noise.},
   author = {Hwanjun Song and Minseok Kim and Dongmin Park and Jae-Gil Lee},
   title = {How does Early Stopping Help Generalization against Label Noise?},
   year = {2020}
}
